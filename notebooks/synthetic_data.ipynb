{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from data.synthetic_data import SyntheticDataset\n",
    "from model import MLP, SharedBottom, OMOE, MMOE, CGC\n",
    "from utils import loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "num_data = 10000\n",
    "input_dim = 100\n",
    "task_corr_list = [0.25, 0.5, 0.75, 1]\n",
    "\n",
    "test_ratio  = 0.1\n",
    "test_size  = int(num_data * test_ratio)\n",
    "train_size = num_data - test_size\n",
    "\n",
    "n_tasks = 2\n",
    "n_experts = 8\n",
    "n_shared_experts = 2\n",
    "n_task_experts = [3,3]\n",
    "expert_size = 16\n",
    "tower_size = 8\n",
    "total_param_size = (input_dim * expert_size * n_experts) + (expert_size * tower_size * n_tasks)\n",
    "shared_size = round(total_param_size / (input_dim + tower_size * n_tasks))\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_epochs = 10\n",
    "mb_size = 10\n",
    "num_mb  = num_data // mb_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "mtl_name_list = ['shared_bottom', 'omoe', 'mmoe', 'cgc']\n",
    "\n",
    "def init_model(model_type):\n",
    "    if model_type == 'single':\n",
    "        model = MLP(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=round(total_param_size/input_dim)\n",
    "        )\n",
    "    elif model_type == 'shared_bottom':\n",
    "        model = SharedBottom(\n",
    "            input_size=input_dim,\n",
    "            shared_size=shared_size,\n",
    "            tower_size=tower_size,\n",
    "            num_tasks=n_tasks\n",
    "        )\n",
    "    elif model_type == 'omoe':\n",
    "        model = OMOE(\n",
    "            input_size=input_dim,\n",
    "            expert_size=expert_size,\n",
    "            tower_size=tower_size,\n",
    "            num_tasks=n_tasks,\n",
    "            num_experts=n_experts\n",
    "        )\n",
    "    elif model_type == 'mmoe':\n",
    "        model = MMOE(\n",
    "            input_size=input_dim,\n",
    "            expert_size=expert_size,\n",
    "            tower_size=tower_size,\n",
    "            num_tasks=n_tasks,\n",
    "            num_experts=n_experts\n",
    "        )\n",
    "    elif model_type == 'cgc':\n",
    "        model = CGC(\n",
    "            input_size=input_dim, \n",
    "            expert_size=expert_size, \n",
    "            tower_size=tower_size, \n",
    "            num_tasks=n_tasks, \n",
    "            num_shared_experts=n_shared_experts, \n",
    "            num_task_experts=n_task_experts\n",
    "        )\n",
    "    \n",
    "    optims = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, optims\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "mtl_loss = loss.MultiTaskLoss()\n",
    "\n",
    "model_dict   = dict()\n",
    "history_dict = dict()\n",
    "for task_corr in task_corr_list:\n",
    "    print(f'Create dataset with task correlation {task_corr}..')\n",
    "    dataset = SyntheticDataset(num_data, input_dim, task_corr=task_corr)\n",
    "    train, test = torch.utils.data.random_split(\n",
    "        dataset, [train_size, test_size]\n",
    "    )\n",
    "\n",
    "    print(f'Initialize model..')\n",
    "    model_st1,  optims_st1  = init_model('single')\n",
    "    model_st2,  optims_st2  = init_model('single')\n",
    "    model_sb,   optims_sb   = init_model('shared_bottom')\n",
    "    model_omoe, optims_omoe = init_model('omoe')\n",
    "    model_mmoe, optims_mmoe = init_model('mmoe')\n",
    "    model_cgc,  optims_cgc  = init_model('cgc')\n",
    "\n",
    "    history = defaultdict(list)\n",
    "    for it in range(n_epochs):\n",
    "        print(f'Start {it}-th epoch..')\n",
    "\n",
    "        cost_st1  = 0\n",
    "        cost_st2  = 0\n",
    "        cost_sb   = 0\n",
    "        cost_omoe = 0\n",
    "        cost_mmoe = 0\n",
    "        cost_cgc  = 0\n",
    "\n",
    "        dataloader = DataLoader(train.dataset, batch_size=mb_size, shuffle=True)\n",
    "        for data in dataloader:\n",
    "            X, y   = data[0], data[1]\n",
    "            y_st1, y_st2 = y[0], y[1]\n",
    "\n",
    "            yhat_st1 = model_st1(X)\n",
    "            loss_st1 = mse_loss(yhat_st1, y_st1.view(-1,1))\n",
    "            optims_st1.zero_grad()\n",
    "            loss_st1.backward()\n",
    "            optims_st1.step()\n",
    "\n",
    "            yhat_st2 = model_st2(X)\n",
    "            loss_st2 = mse_loss(yhat_st2, y_st2.view(-1,1))\n",
    "            optims_st2.zero_grad()\n",
    "            loss_st2.backward()\n",
    "            optims_st2.step()\n",
    "\n",
    "            yhat_sb = model_sb(X)\n",
    "            loss_sb = mtl_loss(yhat_sb, y)\n",
    "            optims_sb.zero_grad()\n",
    "            loss_sb.backward()\n",
    "            optims_sb.step()\n",
    "\n",
    "            yhat_omoe = model_omoe(X)\n",
    "            loss_omoe = mtl_loss(yhat_omoe, y)\n",
    "            optims_omoe.zero_grad()\n",
    "            loss_omoe.backward()\n",
    "            optims_omoe.step()\n",
    "\n",
    "            yhat_mmoe = model_mmoe(X)\n",
    "            loss_mmoe = mtl_loss(yhat_mmoe, y)\n",
    "            optims_mmoe.zero_grad()\n",
    "            loss_mmoe.backward()\n",
    "            optims_mmoe.step()\n",
    "\n",
    "            yhat_cgc = model_cgc(X)\n",
    "            loss_cgc = mtl_loss(yhat_cgc, y)\n",
    "            optims_cgc.zero_grad()\n",
    "            loss_cgc.backward()\n",
    "            optims_cgc.step()\n",
    "\n",
    "            cost_st1  += (loss_st1  / num_mb)\n",
    "            cost_st2  += (loss_st2  / num_mb)\n",
    "            cost_sb   += (loss_sb   / num_mb)\n",
    "            cost_omoe += (loss_omoe / num_mb)\n",
    "            cost_mmoe += (loss_mmoe / num_mb)\n",
    "            cost_cgc  += (loss_cgc  / num_mb)\n",
    "        \n",
    "        history['single_task1'].append(cost_st1.item())\n",
    "        history['single_task2'].append(cost_st2.item())\n",
    "        history['shared_bottom'].append(cost_sb.item())\n",
    "        history['omoe'].append(cost_omoe.item())\n",
    "        history['mmoe'].append(cost_mmoe.item())\n",
    "        history['cgc'].append(cost_cgc.item())\n",
    "        \n",
    "    history_dict[task_corr] = history\n",
    "    model_dict[task_corr] = {\n",
    "        'single_task1' : model_st1, \n",
    "        'single_task2' : model_st2, \n",
    "        'shared_bottom': model_sb, \n",
    "        'omoe': model_omoe, \n",
    "        'mmoe': model_mmoe, \n",
    "        'cgc' : model_cgc\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, axs = plt.subplots(1, len(task_corr_list), sharey=True, figsize=(15, 3))\n",
    "plt.suptitle('MTL Model Loss', fontsize=15, y=1.15)\n",
    "for idx, task_corr in enumerate(task_corr_list):\n",
    "    history = history_dict[task_corr]\n",
    "    model_names = list(history.keys())\n",
    "\n",
    "    ax = axs[idx]\n",
    "    for mtl in mtl_name_list:\n",
    "        ax.plot(history[mtl], label=mtl)\n",
    "    ax.set_title(f'Correlation {task_corr}')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='epoch', ylabel='loss')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.legend(loc=(-2.3, 1.13), ncol=len(model_names))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test = test.dataset.X\n",
    "y1_test = test.dataset.y1.view(-1,1)\n",
    "y2_test = test.dataset.y2.view(-1,1)\n",
    "\n",
    "fig, axs = plt.subplots(1, len(task_corr_list), sharey=True, figsize=(15, 3))\n",
    "plt.suptitle('MTL gain on Synthetic Data', fontsize=15, y=1.15)\n",
    "labels = ['task1', 'task2']\n",
    "width = 0.35\n",
    "x = np.arange(len(labels))\n",
    "\n",
    "for idx, task_corr in enumerate(task_corr_list):\n",
    "    models = model_dict[task_corr]\n",
    "    mtl_gain1_list = []\n",
    "    mtl_gain2_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        yhat_st1 = models['single_task1'](X_test)\n",
    "        yhat_st2 = models['single_task2'](X_test)\n",
    "        mse_st1 = mse_loss(yhat_st1, y1_test)\n",
    "        mse_st2 = mse_loss(yhat_st2, y2_test)\n",
    "\n",
    "        for mtl in mtl_name_list:\n",
    "            yhat1, yhat2 = models[mtl](X_test)\n",
    "            mse1 = mse_loss(yhat1, y1_test)\n",
    "            mse2 = mse_loss(yhat2, y2_test)\n",
    "            mtl_gain1_list.append(mse_st1 - mse1)\n",
    "            mtl_gain2_list.append(mse_st2 - mse2)\n",
    "\n",
    "    ax = axs[idx]\n",
    "    for i, m in enumerate(mtl_name_list):\n",
    "        rects = ax.bar(x - width/2, [mtl_gain1_list[i], mtl_gain2_list[i]], width, label=m)\n",
    "    ax.set_title(f'Correlation {task_corr}')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(ylabel='MTL Gain')\n",
    "    ax.grid(True)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "\n",
    "plt.legend(loc=(-2.3, 1.13), ncol=len(model_names))\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('mtl': conda)"
  },
  "interpreter": {
   "hash": "398376e89c0e13c0cc2d3ce93525f2fc57ea81caa8ddc47c40586bd652337df4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}