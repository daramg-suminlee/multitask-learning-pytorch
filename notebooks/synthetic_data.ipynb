{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "from data.synthetic_data import SyntheticDataset\n",
    "from model import MLP, SharedBottom, OMOE, MMOE, CGC\n",
    "from utils import loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "num_data = 10000\n",
    "input_dim = 100\n",
    "task_corr = 0.5\n",
    "\n",
    "dataset = SyntheticDataset(num_data, input_dim, task_corr=task_corr)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "test_ratio  = 0.1\n",
    "\n",
    "test_size  = int(len(dataset) * test_ratio)\n",
    "train_size = len(dataset) - test_size\n",
    "train, test = torch.utils.data.random_split(\n",
    "    dataset, [train_size, test_size]\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "n_tasks = 2\n",
    "n_experts = 8\n",
    "n_shared_experts = 2\n",
    "n_task_experts = [3,3]\n",
    "expert_size = 16\n",
    "tower_size = 8\n",
    "total_param_size = (input_dim * expert_size * n_experts) + (expert_size * tower_size * n_tasks)\n",
    "shared_size = round(total_param_size / (input_dim + tower_size * n_tasks))\n",
    "\n",
    "model_st1 = MLP(\n",
    "    input_size=input_dim,\n",
    "    hidden_size=round(total_param_size/input_dim)\n",
    ")\n",
    "model_st2 = MLP(\n",
    "    input_size=input_dim,\n",
    "    hidden_size=round(total_param_size/input_dim)\n",
    ")\n",
    "model_sb = SharedBottom(\n",
    "    input_size=input_dim,\n",
    "    shared_size=shared_size,\n",
    "    tower_size=tower_size,\n",
    "    num_tasks=n_tasks\n",
    ")\n",
    "model_omoe = OMOE(\n",
    "    input_size=input_dim,\n",
    "    expert_size=expert_size,\n",
    "    tower_size=tower_size,\n",
    "    num_tasks=n_tasks,\n",
    "    num_experts=n_experts\n",
    ")\n",
    "model_mmoe = MMOE(\n",
    "    input_size=input_dim,\n",
    "    expert_size=expert_size,\n",
    "    tower_size=tower_size,\n",
    "    num_tasks=n_tasks,\n",
    "    num_experts=n_experts\n",
    ")\n",
    "model_cgc = CGC(\n",
    "    input_size=input_dim, \n",
    "    expert_size=expert_size, \n",
    "    tower_size=tower_size, \n",
    "    num_tasks=2, \n",
    "    num_shared_experts=n_shared_experts, \n",
    "    num_task_experts=n_task_experts\n",
    ")\n",
    "\n",
    "lr_list = [0.0001, 0.001, 0.01]\n",
    "optims_st1  = [torch.optim.Adam(model_st1.parameters(),  lr=lr) for lr in lr_list]\n",
    "optims_st2  = [torch.optim.Adam(model_st2.parameters(),  lr=lr) for lr in lr_list]\n",
    "optims_sb   = [torch.optim.Adam(model_sb.parameters(),   lr=lr) for lr in lr_list]\n",
    "optims_omoe = [torch.optim.Adam(model_omoe.parameters(), lr=lr) for lr in lr_list]\n",
    "optims_mmoe = [torch.optim.Adam(model_mmoe.parameters(), lr=lr) for lr in lr_list]\n",
    "optims_cgc  = [torch.optim.Adam(model_cgc.parameters(), lr=lr) for lr in lr_list]\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "mtl_loss = loss.MultiTaskLoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "n_epochs = 10\n",
    "mb_size = 10\n",
    "num_mb  = num_data // mb_size\n",
    "\n",
    "lr_idx = 1\n",
    "for it in range(n_epochs):\n",
    "    cost_st1, cost_st2, cost_sb, cost_omoe, cost_mmoe, cost_cgc = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    dataloader = DataLoader(train.dataset, batch_size=mb_size, shuffle=True)\n",
    "    for data in dataloader:\n",
    "        X_mb, y_mb   = data[0], data[1]\n",
    "        y1_mb, y2_mb = y_mb[0], y_mb[1]\n",
    "\n",
    "        yhat_mb_st1 = model_st1(X_mb)\n",
    "        loss_mb_st1 = mse_loss(yhat_mb_st1, y1_mb.view(-1,1))\n",
    "        optims_st1[lr_idx].zero_grad()\n",
    "        loss_mb_st1.backward()\n",
    "        optims_st1[lr_idx].step()\n",
    "\n",
    "        yhat_mb_st2 = model_st2(X_mb)\n",
    "        loss_mb_st2 = mse_loss(yhat_mb_st2, y2_mb.view(-1,1))\n",
    "        optims_st2[lr_idx].zero_grad()\n",
    "        loss_mb_st2.backward()\n",
    "        optims_st2[lr_idx].step()\n",
    "\n",
    "        yhat_mb_sb = model_sb(X_mb)\n",
    "        loss_mb_sb = mtl_loss(yhat_mb_sb, y_mb)\n",
    "        optims_sb[lr_idx].zero_grad()\n",
    "        loss_mb_sb.backward()\n",
    "        optims_sb[lr_idx].step()\n",
    "\n",
    "        yhat_mb_omoe = model_omoe(X_mb)\n",
    "        loss_mb_omoe = mtl_loss(yhat_mb_omoe, y_mb)\n",
    "        optims_omoe[lr_idx].zero_grad()\n",
    "        loss_mb_omoe.backward()\n",
    "        optims_omoe[lr_idx].step()\n",
    "\n",
    "        yhat_mb_mmoe = model_mmoe(X_mb)\n",
    "        loss_mb_mmoe = mtl_loss(yhat_mb_mmoe, y_mb)\n",
    "        optims_mmoe[lr_idx].zero_grad()\n",
    "        loss_mb_mmoe.backward()\n",
    "        optims_mmoe[lr_idx].step()\n",
    "\n",
    "        yhat_mb_cgc = model_cgc(X_mb)\n",
    "        loss_mb_cgc = mtl_loss(yhat_mb_cgc, y_mb)\n",
    "        optims_cgc[lr_idx].zero_grad()\n",
    "        loss_mb_cgc.backward()\n",
    "        optims_cgc[lr_idx].step()\n",
    "\n",
    "        cost_st1 += (loss_mb_st1   / num_mb)\n",
    "        cost_st2 += (loss_mb_st2   / num_mb)\n",
    "        cost_sb  += (loss_mb_sb    / num_mb)\n",
    "        cost_omoe += (loss_mb_omoe / num_mb)\n",
    "        cost_mmoe += (loss_mb_mmoe / num_mb)\n",
    "        cost_cgc += (loss_mb_cgc / num_mb)\n",
    "    \n",
    "    print(f'[{it}] st1 {cost_st1:.3f}; st2 {cost_st2:.3f}; sb: {cost_sb:.3f}; omoe: {cost_omoe:.3f}; mmoe: {cost_mmoe:.3f}; cgc: {cost_cgc:.3f}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0] st1 0.178; st2 0.171; sb: 0.389; omoe: 0.594; mmoe: 0.556; cgc: 0.498\n",
      "[1] st1 0.145; st2 0.133; sb: 0.283; omoe: 0.486; mmoe: 0.442; cgc: 0.407\n",
      "[2] st1 0.144; st2 0.132; sb: 0.278; omoe: 0.483; mmoe: 0.422; cgc: 0.385\n",
      "[3] st1 0.143; st2 0.131; sb: 0.276; omoe: 0.459; mmoe: 0.395; cgc: 0.356\n",
      "[4] st1 0.143; st2 0.131; sb: 0.275; omoe: 0.419; mmoe: 0.385; cgc: 0.345\n",
      "[5] st1 0.143; st2 0.131; sb: 0.275; omoe: 0.401; mmoe: 0.381; cgc: 0.317\n",
      "[6] st1 0.143; st2 0.131; sb: 0.275; omoe: 0.388; mmoe: 0.339; cgc: 0.291\n",
      "[7] st1 0.143; st2 0.131; sb: 0.275; omoe: 0.374; mmoe: 0.312; cgc: 0.282\n",
      "[8] st1 0.142; st2 0.131; sb: 0.274; omoe: 0.328; mmoe: 0.296; cgc: 0.278\n",
      "[9] st1 0.142; st2 0.131; sb: 0.274; omoe: 0.297; mmoe: 0.288; cgc: 0.280\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "with torch.no_grad():\n",
    "    yhat_st1 = model_st1(test.dataset.X)\n",
    "    yhat_st2 = model_st2(test.dataset.X)\n",
    "    yhat_sb  = model_sb(test.dataset.X)\n",
    "    yhat_omoe = model_omoe(test.dataset.X)\n",
    "    yhat_mmoe = model_mmoe(test.dataset.X)\n",
    "    yhat_cgc = model_cgc(test.dataset.X)\n",
    "    print(f'single task:   {mse_loss(yhat_st1, test.dataset.y1.view(-1,1)).item():.3f}, {mse_loss(yhat_st2, test.dataset.y2.view(-1,1)).item():.3f}')\n",
    "    print(f'shared bottom: {mse_loss(yhat_sb[0], test.dataset.y1.view(-1,1)).item():.3f}, {mse_loss(yhat_sb[1], test.dataset.y2.view(-1,1)).item():.3f}')\n",
    "    print(f'onegate moe:   {mse_loss(yhat_omoe[0], test.dataset.y1.view(-1,1)).item():.3f}, {mse_loss(yhat_omoe[1], test.dataset.y2.view(-1,1)).item():.3f}')\n",
    "    print(f'multigate moe: {mse_loss(yhat_mmoe[0], test.dataset.y1.view(-1,1)).item():.3f}, {mse_loss(yhat_mmoe[1], test.dataset.y2.view(-1,1)).item():.3f}')\n",
    "    print(f'cgc:           {mse_loss(yhat_cgc[0], test.dataset.y1.view(-1,1)).item():.3f}, {mse_loss(yhat_cgc[1], test.dataset.y2.view(-1,1)).item():.3f}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "single task:   0.143, 0.130\n",
      "shared bottom: 0.143, 0.131\n",
      "onegate moe:   0.153, 0.135\n",
      "multigate moe: 0.146, 0.140\n",
      "cgc:           0.147, 0.133\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('mtl': conda)"
  },
  "interpreter": {
   "hash": "5cd247a6b981a037365163265c57d056e9607e35ce1ab77e4c3c04ffb4992aeb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}